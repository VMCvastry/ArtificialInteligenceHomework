{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TspH1wJmAkdk"
   },
   "source": [
    "### For the homeworks we are going to use the \"[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)\"\n",
    "\n",
    "The dataset can be used both for regression and classification tasks.\n",
    "\n",
    "#### Source:\n",
    "\n",
    "Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.\n",
    "Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
    "Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
    "Pedro Sernadela Universidade de Aveiro\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\n",
    "* Acquisition date: January 8, 2015\n",
    "* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n",
    "\n",
    "Attribute Information:\n",
    "0. url: URL of the article (non-predictive)\n",
    "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "2. n_tokens_title: Number of words in the title\n",
    "3. n_tokens_content: Number of words in the content\n",
    "4. n_unique_tokens: Rate of unique words in the content\n",
    "5. n_non_stop_words: Rate of non-stop words in the content\n",
    "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "7. num_hrefs: Number of links\n",
    "8. num_self_hrefs: Number of links to other articles published by Mashable\n",
    "9. num_imgs: Number of images\n",
    "10. num_videos: Number of videos\n",
    "11. average_token_length: Average length of the words in the content\n",
    "12. num_keywords: Number of keywords in the metadata\n",
    "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "14. data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "15. data_channel_is_bus: Is data channel 'Business'?\n",
    "16. data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "17. data_channel_is_tech: Is data channel 'Tech'?\n",
    "18. data_channel_is_world: Is data channel 'World'?\n",
    "19. kw_min_min: Worst keyword (min. shares)\n",
    "20. kw_max_min: Worst keyword (max. shares)\n",
    "21. kw_avg_min: Worst keyword (avg. shares)\n",
    "22. kw_min_max: Best keyword (min. shares)\n",
    "23. kw_max_max: Best keyword (max. shares)\n",
    "24. kw_avg_max: Best keyword (avg. shares)\n",
    "25. kw_min_avg: Avg. keyword (min. shares)\n",
    "26. kw_max_avg: Avg. keyword (max. shares)\n",
    "27. kw_avg_avg: Avg. keyword (avg. shares)\n",
    "28. self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "29. self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "31. weekday_is_monday: Was the article published on a Monday?\n",
    "32. weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "33. weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "34. weekday_is_thursday: Was the article published on a Thursday?\n",
    "35. weekday_is_friday: Was the article published on a Friday?\n",
    "36. weekday_is_saturday: Was the article published on a Saturday?\n",
    "37. weekday_is_sunday: Was the article published on a Sunday?\n",
    "38. is_weekend: Was the article published on the weekend?\n",
    "39. LDA_00: Closeness to LDA topic 0\n",
    "40. LDA_01: Closeness to LDA topic 1\n",
    "41. LDA_02: Closeness to LDA topic 2\n",
    "42. LDA_03: Closeness to LDA topic 3\n",
    "43. LDA_04: Closeness to LDA topic 4\n",
    "44. global_subjectivity: Text subjectivity\n",
    "45. global_sentiment_polarity: Text sentiment polarity\n",
    "46. global_rate_positive_words: Rate of positive words in the content\n",
    "47. global_rate_negative_words: Rate of negative words in the content\n",
    "48. rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "49. rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "50. avg_positive_polarity: Avg. polarity of positive words\n",
    "51. min_positive_polarity: Min. polarity of positive words\n",
    "52. max_positive_polarity: Max. polarity of positive words\n",
    "53. avg_negative_polarity: Avg. polarity of negative words\n",
    "54. min_negative_polarity: Min. polarity of negative words\n",
    "55. max_negative_polarity: Max. polarity of negative words\n",
    "56. title_subjectivity: Title subjectivity\n",
    "57. title_sentiment_polarity: Title polarity\n",
    "58. abs_title_subjectivity: Absolute subjectivity level\n",
    "59. abs_title_sentiment_polarity: Absolute polarity level\n",
    "60. shares: Number of shares (target)\n",
    "\n",
    "\n",
    "The first two columns (url and time_delta) are non-predictive and should be ignored\n",
    "\n",
    "The last column **shares** contains the value to predict.\n",
    "\n",
    "### Regression\n",
    "In the case of regression we want to predict the value of the share column.\n",
    "\n",
    "### Classification\n",
    "In the case of classification we want to predict one of two classes:\n",
    "\n",
    "* *low* -- shares < 1,400\n",
    "* *high* -- shares >= 1,400\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Regression\n",
    "To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\Big(d_i -f_i\\Big)^2}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $n$ is the number of test samples\n",
    "* $d_i$ is the ground truth value of the i-th sample\n",
    "* $f_i$ is the predicted value of the i-th sample\n",
    "\n",
    "\n",
    "#### Classification\n",
    "To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by\n",
    "\n",
    "$$\n",
    "\\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* TP is the number of *correctly* classified positive samples\n",
    "* TN is the number of *correctly* classified negative samples\n",
    "* FP is the number of *incorrectly* classified positive samples\n",
    "* FN is the number of *incorrectly* classified negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oesd6_bYijRo",
    "outputId": "a840832a-3520-47e8-9939-6224b0bb241b"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmad6QdZ_nFR",
    "outputId": "bfbf18fd-0edd-4424-b696-f48ccff3a4a6"
   },
   "outputs": [],
   "source": [
    "# !wget https: // archive.ics.uci.edu / ml / machine-learning-databases / 00332 / OnlineNewsPopularity.zip\n",
    "# !unzip OnlineNewsPopularity.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Format properly the names of the columns and remove the first two columns"
   ],
   "metadata": {
    "id": "yXCX_LpFedtj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "BINARY_LABEL = False\n",
    "NORMALIZE = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1mxntjhmAH0D"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "df = df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's plot some of the columns"
   ],
   "metadata": {
    "id": "mu_rxzq0f2gV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VbFlnYRjAXlk",
    "outputId": "e9048aa7-7f2f-4e22-b550-92237f8d5512"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "columns_to_plot = [\n",
    "    'n_tokens_title',\n",
    "    'num_videos',\n",
    "    'num_imgs',\n",
    "    'num_keywords',\n",
    "    'data_channel_is_world',\n",
    "    'rate_negative_words',\n",
    "    'self_reference_avg_sharess',\n",
    "]\n",
    "#\n",
    "# fig, ax = plt.subplots(len(columns_to_plot), 1, figsize=(20, 20))\n",
    "#\n",
    "# for i, column in enumerate(columns_to_plot, 0):\n",
    "#     ax[i].hist(df[column])\n",
    "#     ax[i].title.set_text(column)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title                       10.398749\n",
      "n_tokens_content                    546.514731\n",
      "n_unique_tokens                       0.548216\n",
      "n_non_stop_words                      0.996469\n",
      "n_non_stop_unique_tokens              0.689175\n",
      "num_hrefs                            10.883690\n",
      "num_self_hrefs                        3.293638\n",
      "num_imgs                              4.544143\n",
      "num_videos                            1.249874\n",
      "average_token_length                  4.548239\n",
      "num_keywords                          7.223767\n",
      "data_channel_is_lifestyle             0.052946\n",
      "data_channel_is_entertainment         0.178009\n",
      "data_channel_is_bus                   0.157855\n",
      "data_channel_is_socmed                0.058597\n",
      "data_channel_is_tech                  0.185299\n",
      "data_channel_is_world                 0.212567\n",
      "kw_min_min                           26.106801\n",
      "kw_max_min                         1153.951682\n",
      "kw_avg_min                          312.366967\n",
      "kw_min_max                        13612.354102\n",
      "kw_max_max                       752324.066694\n",
      "kw_avg_max                       259281.938083\n",
      "kw_min_avg                         1117.146610\n",
      "kw_max_avg                         5657.211151\n",
      "kw_avg_avg                         3135.858639\n",
      "self_reference_min_shares          3998.755396\n",
      "self_reference_max_shares         10329.212662\n",
      "self_reference_avg_sharess         6401.697580\n",
      "weekday_is_monday                     0.168020\n",
      "weekday_is_tuesday                    0.186409\n",
      "weekday_is_wednesday                  0.187544\n",
      "weekday_is_thursday                   0.183306\n",
      "weekday_is_friday                     0.143805\n",
      "weekday_is_saturday                   0.061876\n",
      "weekday_is_sunday                     0.069039\n",
      "is_weekend                            0.130915\n",
      "LDA_00                                0.184599\n",
      "LDA_01                                0.141256\n",
      "LDA_02                                0.216321\n",
      "LDA_03                                0.223770\n",
      "LDA_04                                0.234029\n",
      "global_subjectivity                   0.443370\n",
      "global_sentiment_polarity             0.119309\n",
      "global_rate_positive_words            0.039625\n",
      "global_rate_negative_words            0.016612\n",
      "rate_positive_words                   0.682150\n",
      "rate_negative_words                   0.287934\n",
      "avg_positive_polarity                 0.353825\n",
      "min_positive_polarity                 0.095446\n",
      "max_positive_polarity                 0.756728\n",
      "avg_negative_polarity                -0.259524\n",
      "min_negative_polarity                -0.521944\n",
      "max_negative_polarity                -0.107500\n",
      "title_subjectivity                    0.282353\n",
      "title_sentiment_polarity              0.071425\n",
      "abs_title_subjectivity                0.341843\n",
      "abs_title_sentiment_polarity          0.156064\n",
      "shares                             3395.380184\n",
      "dtype: float64\n",
      "n_tokens_title                       10.000000\n",
      "n_tokens_content                    409.000000\n",
      "n_unique_tokens                       0.539226\n",
      "n_non_stop_words                      1.000000\n",
      "n_non_stop_unique_tokens              0.690476\n",
      "num_hrefs                             8.000000\n",
      "num_self_hrefs                        3.000000\n",
      "num_imgs                              1.000000\n",
      "num_videos                            0.000000\n",
      "average_token_length                  4.664082\n",
      "num_keywords                          7.000000\n",
      "data_channel_is_lifestyle             0.000000\n",
      "data_channel_is_entertainment         0.000000\n",
      "data_channel_is_bus                   0.000000\n",
      "data_channel_is_socmed                0.000000\n",
      "data_channel_is_tech                  0.000000\n",
      "data_channel_is_world                 0.000000\n",
      "kw_min_min                           -1.000000\n",
      "kw_max_min                          660.000000\n",
      "kw_avg_min                          235.500000\n",
      "kw_min_max                         1400.000000\n",
      "kw_max_max                       843300.000000\n",
      "kw_avg_max                       244572.222223\n",
      "kw_min_avg                         1023.635611\n",
      "kw_max_avg                         4355.688836\n",
      "kw_avg_avg                         2870.074878\n",
      "self_reference_min_shares          1200.000000\n",
      "self_reference_max_shares          2800.000000\n",
      "self_reference_avg_sharess         2200.000000\n",
      "weekday_is_monday                     0.000000\n",
      "weekday_is_tuesday                    0.000000\n",
      "weekday_is_wednesday                  0.000000\n",
      "weekday_is_thursday                   0.000000\n",
      "weekday_is_friday                     0.000000\n",
      "weekday_is_saturday                   0.000000\n",
      "weekday_is_sunday                     0.000000\n",
      "is_weekend                            0.000000\n",
      "LDA_00                                0.033387\n",
      "LDA_01                                0.033345\n",
      "LDA_02                                0.040004\n",
      "LDA_03                                0.040001\n",
      "LDA_04                                0.040727\n",
      "global_subjectivity                   0.453457\n",
      "global_sentiment_polarity             0.119117\n",
      "global_rate_positive_words            0.039023\n",
      "global_rate_negative_words            0.015337\n",
      "rate_positive_words                   0.710526\n",
      "rate_negative_words                   0.280000\n",
      "avg_positive_polarity                 0.358755\n",
      "min_positive_polarity                 0.100000\n",
      "max_positive_polarity                 0.800000\n",
      "avg_negative_polarity                -0.253333\n",
      "min_negative_polarity                -0.500000\n",
      "max_negative_polarity                -0.100000\n",
      "title_subjectivity                    0.150000\n",
      "title_sentiment_polarity              0.000000\n",
      "abs_title_subjectivity                0.500000\n",
      "abs_title_sentiment_polarity          0.000000\n",
      "shares                             1400.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#calculate median for each column\n",
    "median = df.median(axis=0)\n",
    "avg = df.mean(axis=0)\n",
    "print(avg)\n",
    "print(median)\n",
    "# compute the median of each attribute\n",
    "medians = df.median()\n",
    "\n",
    "# discretize each attribute to 0 or 1 based on the median\n",
    "# for column in df.columns:\n",
    "#     df[column] = (df[column] >= medians[column]).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = np.array(df)\n",
    "if NORMALIZE:\n",
    "    # normalize the data\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "x = data[:, :-1]\n",
    "# converting the last column to boolean\n",
    "if BINARY_LABEL:\n",
    "    assert not NORMALIZE\n",
    "    y = np.array([elem >= 1400 for elem in data[:, -1]])\n",
    "else:\n",
    "    y = np.array(data[:, -1])\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "dLIoAQQY-ql_",
    "outputId": "bf5d14ac-b0f6-410e-98cd-d69cea64e8ea"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  0.11259913444519043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regressor = LinearRegression()\n",
    "time_start = time.time()\n",
    "regressor.fit(train_x, train_y)\n",
    "print(\"Time taken to train the model: \", time.time() - time_start)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def analyze_pred(pred, truth):\n",
    "    # Print the mean squared error and R-squared score\n",
    "    print('Mean Squared Error:', mean_squared_error(truth, pred))\n",
    "    print('R-squared Score:', r2_score(test_y, pred))\n",
    "    print(np.mean(pred))\n",
    "    print(np.mean(truth))\n",
    "    bin_pred = pred >= 0\n",
    "    bin_truth = truth >= 0\n",
    "    print('Accuracy:', accuracy_score(bin_truth, bin_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5051645325256792\n",
      "R-squared Score: 0.03273223844671258\n",
      "0.006017241700748466\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.6391726573338378\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "pred = regressor.predict(test_x)\n",
    "\n",
    "# Print the regression coefficients and intercept\n",
    "# print('Coefficients:', regressor.coef_)\n",
    "# print('Intercept:', regressor.intercept_)\n",
    "analyze_pred(pred, test_y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression From Scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ScratchLinearRegression:\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        pass#%% md\n",
    "### For the homeworks we are going to use the \"[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)\"\n",
    "\n",
    "The dataset can be used both for regression and classification tasks.\n",
    "\n",
    "#### Source:\n",
    "\n",
    "Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.\n",
    "Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
    "Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
    "Pedro Sernadela Universidade de Aveiro\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\n",
    "* Acquisition date: January 8, 2015\n",
    "* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n",
    "\n",
    "Attribute Information:\n",
    "0. url: URL of the article (non-predictive)\n",
    "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "2. n_tokens_title: Number of words in the title\n",
    "3. n_tokens_content: Number of words in the content\n",
    "4. n_unique_tokens: Rate of unique words in the content\n",
    "5. n_non_stop_words: Rate of non-stop words in the content\n",
    "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "7. num_hrefs: Number of links\n",
    "8. num_self_hrefs: Number of links to other articles published by Mashable\n",
    "9. num_imgs: Number of images\n",
    "10. num_videos: Number of videos\n",
    "11. average_token_length: Average length of the words in the content\n",
    "12. num_keywords: Number of keywords in the metadata\n",
    "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "14. data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "15. data_channel_is_bus: Is data channel 'Business'?\n",
    "16. data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "17. data_channel_is_tech: Is data channel 'Tech'?\n",
    "18. data_channel_is_world: Is data channel 'World'?\n",
    "19. kw_min_min: Worst keyword (min. shares)\n",
    "20. kw_max_min: Worst keyword (max. shares)\n",
    "21. kw_avg_min: Worst keyword (avg. shares)\n",
    "22. kw_min_max: Best keyword (min. shares)\n",
    "23. kw_max_max: Best keyword (max. shares)\n",
    "24. kw_avg_max: Best keyword (avg. shares)\n",
    "25. kw_min_avg: Avg. keyword (min. shares)\n",
    "26. kw_max_avg: Avg. keyword (max. shares)\n",
    "27. kw_avg_avg: Avg. keyword (avg. shares)\n",
    "28. self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "29. self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "31. weekday_is_monday: Was the article published on a Monday?\n",
    "32. weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "33. weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "34. weekday_is_thursday: Was the article published on a Thursday?\n",
    "35. weekday_is_friday: Was the article published on a Friday?\n",
    "36. weekday_is_saturday: Was the article published on a Saturday?\n",
    "37. weekday_is_sunday: Was the article published on a Sunday?\n",
    "38. is_weekend: Was the article published on the weekend?\n",
    "39. LDA_00: Closeness to LDA topic 0\n",
    "40. LDA_01: Closeness to LDA topic 1\n",
    "41. LDA_02: Closeness to LDA topic 2\n",
    "42. LDA_03: Closeness to LDA topic 3\n",
    "43. LDA_04: Closeness to LDA topic 4\n",
    "44. global_subjectivity: Text subjectivity\n",
    "45. global_sentiment_polarity: Text sentiment polarity\n",
    "46. global_rate_positive_words: Rate of positive words in the content\n",
    "47. global_rate_negative_words: Rate of negative words in the content\n",
    "48. rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "49. rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "50. avg_positive_polarity: Avg. polarity of positive words\n",
    "51. min_positive_polarity: Min. polarity of positive words\n",
    "52. max_positive_polarity: Max. polarity of positive words\n",
    "53. avg_negative_polarity: Avg. polarity of negative words\n",
    "54. min_negative_polarity: Min. polarity of negative words\n",
    "55. max_negative_polarity: Max. polarity of negative words\n",
    "56. title_subjectivity: Title subjectivity\n",
    "57. title_sentiment_polarity: Title polarity\n",
    "58. abs_title_subjectivity: Absolute subjectivity level\n",
    "59. abs_title_sentiment_polarity: Absolute polarity level\n",
    "60. shares: Number of shares (target)\n",
    "\n",
    "\n",
    "The first two columns (url and time_delta) are non-predictive and should be ignored\n",
    "\n",
    "The last column **shares** contains the value to predict.\n",
    "\n",
    "### Regression\n",
    "In the case of regression we want to predict the value of the share column.\n",
    "\n",
    "### Classification\n",
    "In the case of classification we want to predict one of two classes:\n",
    "\n",
    "* *low* -- shares < 1,400\n",
    "* *high* -- shares >= 1,400\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Regression\n",
    "To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\Big(d_i -f_i\\Big)^2}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $n$ is the number of test samples\n",
    "* $d_i$ is the ground truth value of the i-th sample\n",
    "* $f_i$ is the predicted value of the i-th sample\n",
    "\n",
    "\n",
    "#### Classification\n",
    "To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by\n",
    "\n",
    "$$\n",
    "\\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* TP is the number of *correctly* classified positive samples\n",
    "* TN is the number of *correctly* classified negative samples\n",
    "* FP is the number of *incorrectly* classified positive samples\n",
    "* FN is the number of *incorrectly* classified negative samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !wget https: // archive.ics.uci.edu / ml / machine-learning-databases / 00332 / OnlineNewsPopularity.zip\n",
    "# !unzip OnlineNewsPopularity.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Format properly the names of the columns and remove the first two columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "BINARY_LABEL = False\n",
    "NORMALIZE = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "df = df.iloc[:, 2:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's plot some of the columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "columns_to_plot = [\n",
    "    'n_tokens_title',\n",
    "    'num_videos',\n",
    "    'num_imgs',\n",
    "    'num_keywords',\n",
    "    'data_channel_is_world',\n",
    "    'rate_negative_words',\n",
    "    'self_reference_avg_sharess',\n",
    "]\n",
    "#\n",
    "# fig, ax = plt.subplots(len(columns_to_plot), 1, figsize=(20, 20))\n",
    "#\n",
    "# for i, column in enumerate(columns_to_plot, 0):\n",
    "#     ax[i].hist(df[column])\n",
    "#     ax[i].title.set_text(column)\n",
    "\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#calculate median for each column\n",
    "median = df.median(axis=0)\n",
    "avg = df.mean(axis=0)\n",
    "print(avg)\n",
    "print(median)\n",
    "# compute the median of each attribute\n",
    "medians = df.median()\n",
    "\n",
    "# discretize each attribute to 0 or 1 based on the median\n",
    "# for column in df.columns:\n",
    "#     df[column] = (df[column] >= medians[column]).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = np.array(df)\n",
    "if NORMALIZE:\n",
    "    # normalize the data\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "x = data[:, :-1]\n",
    "# converting the last column to boolean\n",
    "if BINARY_LABEL:\n",
    "    assert not NORMALIZE\n",
    "    y = np.array([elem >= 1400 for elem in data[:, -1]])\n",
    "else:\n",
    "    y = np.array(data[:, -1])\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def analyze_pred(pred, truth):\n",
    "    # Print the mean squared error and R-squared score\n",
    "    print('Mean Squared Error:', mean_squared_error(truth, pred))\n",
    "    print('R-squared Score:', r2_score(test_y, pred))\n",
    "    print(np.mean(pred))\n",
    "    print(np.mean(truth))\n",
    "    bin_pred = pred >= 0\n",
    "    bin_truth = truth >= 0\n",
    "    print('Accuracy:', accuracy_score(bin_truth, bin_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def test_model(model, train_x, train_y, test_x, test_y):\n",
    "    time_start = time.time()\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Time taken to train the model: \", time.time() - time_start)\n",
    "    pred = model.predict(test_x)\n",
    "    analyze_pred(pred, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  0.10716104507446289\n",
      "Mean Squared Error: 0.5051645325256792\n",
      "R-squared Score: 0.03273223844671258\n",
      "0.006017241700748466\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.6391726573338378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# Print the regression coefficients and intercept\n",
    "# print('Coefficients:', regressor.coef_)\n",
    "# print('Intercept:', regressor.intercept_)\n",
    "test_model(regressor, train_x, train_y, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression From Scratch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class ScratchLinearRegression:\n",
    "\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.dot(X, self.weights) + self.bias\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  3.2577145099639893\n",
      "Mean Squared Error: 0.5058603974087486\n",
      "R-squared Score: 0.031399825689180094\n",
      "0.004790424007427023\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.6432084752175558\n"
     ]
    }
   ],
   "source": [
    "regressor = ScratchLinearRegression()\n",
    "\n",
    "# Print the regression coefficients and intercept\n",
    "# print('Coefficients:', regressor.coef_)\n",
    "# print('Intercept:', regressor.intercept_)\n",
    "test_model(regressor, train_x, train_y, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class ScratchLinearRegressionAnalytical:\n",
    "    def __init__(self, regularization=None):\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "        self.weights = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "        return X_b.dot(self.weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  0.06556487083435059\n",
      "Mean Squared Error: 0.5054352504125945\n",
      "R-squared Score: 0.0322138792436627\n",
      "0.005712651254558729\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.638668180098373\n"
     ]
    }
   ],
   "source": [
    "regressor = ScratchLinearRegressionAnalytical()\n",
    "\n",
    "# Print the regression coefficients and intercept\n",
    "# print('Coefficients:', regressor.coef_)\n",
    "# print('Intercept:', regressor.intercept_)\n",
    "test_model(regressor, train_x, train_y, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  0.18340158462524414\n",
      "Mean Squared Error: 0.5035272892464725\n",
      "R-squared Score: 0.035867162891781046\n",
      "0.004943734584281103\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.6423256400554925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet\n",
    "# alpha = 10\n",
    "# model = Ridge(alpha=alpha)\n",
    "# alpha = 0.01\n",
    "# model = Lasso(alpha=alpha)\n",
    "\n",
    "model = ElasticNet(alpha=0.01,l1_ratio=0.2)\n",
    "test_model(model, train_x, train_y, test_x, test_y)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
