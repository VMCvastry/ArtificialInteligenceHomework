{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TspH1wJmAkdk"
   },
   "source": [
    "### For the homeworks we are going to use the \"[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)\"\n",
    "\n",
    "The dataset can be used both for regression and classification tasks.\n",
    "\n",
    "#### Source:\n",
    "\n",
    "Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.\n",
    "Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
    "Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
    "Pedro Sernadela Universidade de Aveiro\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\n",
    "* Acquisition date: January 8, 2015\n",
    "* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n",
    "\n",
    "Attribute Information:\n",
    "0. url: URL of the article (non-predictive)\n",
    "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "2. n_tokens_title: Number of words in the title\n",
    "3. n_tokens_content: Number of words in the content\n",
    "4. n_unique_tokens: Rate of unique words in the content\n",
    "5. n_non_stop_words: Rate of non-stop words in the content\n",
    "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "7. num_hrefs: Number of links\n",
    "8. num_self_hrefs: Number of links to other articles published by Mashable\n",
    "9. num_imgs: Number of images\n",
    "10. num_videos: Number of videos\n",
    "11. average_token_length: Average length of the words in the content\n",
    "12. num_keywords: Number of keywords in the metadata\n",
    "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "14. data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "15. data_channel_is_bus: Is data channel 'Business'?\n",
    "16. data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "17. data_channel_is_tech: Is data channel 'Tech'?\n",
    "18. data_channel_is_world: Is data channel 'World'?\n",
    "19. kw_min_min: Worst keyword (min. shares)\n",
    "20. kw_max_min: Worst keyword (max. shares)\n",
    "21. kw_avg_min: Worst keyword (avg. shares)\n",
    "22. kw_min_max: Best keyword (min. shares)\n",
    "23. kw_max_max: Best keyword (max. shares)\n",
    "24. kw_avg_max: Best keyword (avg. shares)\n",
    "25. kw_min_avg: Avg. keyword (min. shares)\n",
    "26. kw_max_avg: Avg. keyword (max. shares)\n",
    "27. kw_avg_avg: Avg. keyword (avg. shares)\n",
    "28. self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "29. self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "31. weekday_is_monday: Was the article published on a Monday?\n",
    "32. weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "33. weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "34. weekday_is_thursday: Was the article published on a Thursday?\n",
    "35. weekday_is_friday: Was the article published on a Friday?\n",
    "36. weekday_is_saturday: Was the article published on a Saturday?\n",
    "37. weekday_is_sunday: Was the article published on a Sunday?\n",
    "38. is_weekend: Was the article published on the weekend?\n",
    "39. LDA_00: Closeness to LDA topic 0\n",
    "40. LDA_01: Closeness to LDA topic 1\n",
    "41. LDA_02: Closeness to LDA topic 2\n",
    "42. LDA_03: Closeness to LDA topic 3\n",
    "43. LDA_04: Closeness to LDA topic 4\n",
    "44. global_subjectivity: Text subjectivity\n",
    "45. global_sentiment_polarity: Text sentiment polarity\n",
    "46. global_rate_positive_words: Rate of positive words in the content\n",
    "47. global_rate_negative_words: Rate of negative words in the content\n",
    "48. rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "49. rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "50. avg_positive_polarity: Avg. polarity of positive words\n",
    "51. min_positive_polarity: Min. polarity of positive words\n",
    "52. max_positive_polarity: Max. polarity of positive words\n",
    "53. avg_negative_polarity: Avg. polarity of negative words\n",
    "54. min_negative_polarity: Min. polarity of negative words\n",
    "55. max_negative_polarity: Max. polarity of negative words\n",
    "56. title_subjectivity: Title subjectivity\n",
    "57. title_sentiment_polarity: Title polarity\n",
    "58. abs_title_subjectivity: Absolute subjectivity level\n",
    "59. abs_title_sentiment_polarity: Absolute polarity level\n",
    "60. shares: Number of shares (target)\n",
    "\n",
    "\n",
    "The first two columns (url and time_delta) are non-predictive and should be ignored\n",
    "\n",
    "The last column **shares** contains the value to predict.\n",
    "\n",
    "### Regression\n",
    "In the case of regression we want to predict the value of the share column.\n",
    "\n",
    "### Classification\n",
    "In the case of classification we want to predict one of two classes:\n",
    "\n",
    "* *low* -- shares < 1,400\n",
    "* *high* -- shares >= 1,400\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Regression\n",
    "To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\Big(d_i -f_i\\Big)^2}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $n$ is the number of test samples\n",
    "* $d_i$ is the ground truth value of the i-th sample\n",
    "* $f_i$ is the predicted value of the i-th sample\n",
    "\n",
    "\n",
    "#### Classification\n",
    "To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by\n",
    "\n",
    "$$\n",
    "\\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* TP is the number of *correctly* classified positive samples\n",
    "* TN is the number of *correctly* classified negative samples\n",
    "* FP is the number of *incorrectly* classified positive samples\n",
    "* FN is the number of *incorrectly* classified negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oesd6_bYijRo",
    "outputId": "a840832a-3520-47e8-9939-6224b0bb241b",
    "ExecuteTime": {
     "start_time": "2023-04-04T00:06:02.736852Z",
     "end_time": "2023-04-04T00:06:03.711596Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmad6QdZ_nFR",
    "outputId": "bfbf18fd-0edd-4424-b696-f48ccff3a4a6",
    "ExecuteTime": {
     "start_time": "2023-04-04T00:06:03.713139Z",
     "end_time": "2023-04-04T00:06:03.715771Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https: // archive.ics.uci.edu / ml / machine-learning-databases / 00332 / OnlineNewsPopularity.zip\n",
    "# !unzip OnlineNewsPopularity.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Format properly the names of the columns and remove the first two columns"
   ],
   "metadata": {
    "id": "yXCX_LpFedtj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "BINARY_LABEL = False\n",
    "NORMALIZE = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:15:52.895518Z",
     "end_time": "2023-04-04T00:15:52.940067Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1mxntjhmAH0D",
    "ExecuteTime": {
     "start_time": "2023-04-04T00:06:03.723732Z",
     "end_time": "2023-04-04T00:06:03.993756Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
    "df = df.rename(columns=lambda x: x.strip())\n",
    "df = df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's plot some of the columns"
   ],
   "metadata": {
    "id": "mu_rxzq0f2gV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VbFlnYRjAXlk",
    "outputId": "e9048aa7-7f2f-4e22-b550-92237f8d5512",
    "ExecuteTime": {
     "start_time": "2023-04-03T23:38:08.121503Z",
     "end_time": "2023-04-03T23:38:08.421473Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "columns_to_plot = [\n",
    "    'n_tokens_title',\n",
    "    'num_videos',\n",
    "    'num_imgs',\n",
    "    'num_keywords',\n",
    "    'data_channel_is_world',\n",
    "    'rate_negative_words',\n",
    "    'self_reference_avg_sharess',\n",
    "]\n",
    "#\n",
    "# fig, ax = plt.subplots(len(columns_to_plot), 1, figsize=(20, 20))\n",
    "#\n",
    "# for i, column in enumerate(columns_to_plot, 0):\n",
    "#     ax[i].hist(df[column])\n",
    "#     ax[i].title.set_text(column)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title                       10.398749\n",
      "n_tokens_content                    546.514731\n",
      "n_unique_tokens                       0.548216\n",
      "n_non_stop_words                      0.996469\n",
      "n_non_stop_unique_tokens              0.689175\n",
      "num_hrefs                            10.883690\n",
      "num_self_hrefs                        3.293638\n",
      "num_imgs                              4.544143\n",
      "num_videos                            1.249874\n",
      "average_token_length                  4.548239\n",
      "num_keywords                          7.223767\n",
      "data_channel_is_lifestyle             0.052946\n",
      "data_channel_is_entertainment         0.178009\n",
      "data_channel_is_bus                   0.157855\n",
      "data_channel_is_socmed                0.058597\n",
      "data_channel_is_tech                  0.185299\n",
      "data_channel_is_world                 0.212567\n",
      "kw_min_min                           26.106801\n",
      "kw_max_min                         1153.951682\n",
      "kw_avg_min                          312.366967\n",
      "kw_min_max                        13612.354102\n",
      "kw_max_max                       752324.066694\n",
      "kw_avg_max                       259281.938083\n",
      "kw_min_avg                         1117.146610\n",
      "kw_max_avg                         5657.211151\n",
      "kw_avg_avg                         3135.858639\n",
      "self_reference_min_shares          3998.755396\n",
      "self_reference_max_shares         10329.212662\n",
      "self_reference_avg_sharess         6401.697580\n",
      "weekday_is_monday                     0.168020\n",
      "weekday_is_tuesday                    0.186409\n",
      "weekday_is_wednesday                  0.187544\n",
      "weekday_is_thursday                   0.183306\n",
      "weekday_is_friday                     0.143805\n",
      "weekday_is_saturday                   0.061876\n",
      "weekday_is_sunday                     0.069039\n",
      "is_weekend                            0.130915\n",
      "LDA_00                                0.184599\n",
      "LDA_01                                0.141256\n",
      "LDA_02                                0.216321\n",
      "LDA_03                                0.223770\n",
      "LDA_04                                0.234029\n",
      "global_subjectivity                   0.443370\n",
      "global_sentiment_polarity             0.119309\n",
      "global_rate_positive_words            0.039625\n",
      "global_rate_negative_words            0.016612\n",
      "rate_positive_words                   0.682150\n",
      "rate_negative_words                   0.287934\n",
      "avg_positive_polarity                 0.353825\n",
      "min_positive_polarity                 0.095446\n",
      "max_positive_polarity                 0.756728\n",
      "avg_negative_polarity                -0.259524\n",
      "min_negative_polarity                -0.521944\n",
      "max_negative_polarity                -0.107500\n",
      "title_subjectivity                    0.282353\n",
      "title_sentiment_polarity              0.071425\n",
      "abs_title_subjectivity                0.341843\n",
      "abs_title_sentiment_polarity          0.156064\n",
      "shares                             3395.380184\n",
      "dtype: float64\n",
      "n_tokens_title                       10.000000\n",
      "n_tokens_content                    409.000000\n",
      "n_unique_tokens                       0.539226\n",
      "n_non_stop_words                      1.000000\n",
      "n_non_stop_unique_tokens              0.690476\n",
      "num_hrefs                             8.000000\n",
      "num_self_hrefs                        3.000000\n",
      "num_imgs                              1.000000\n",
      "num_videos                            0.000000\n",
      "average_token_length                  4.664082\n",
      "num_keywords                          7.000000\n",
      "data_channel_is_lifestyle             0.000000\n",
      "data_channel_is_entertainment         0.000000\n",
      "data_channel_is_bus                   0.000000\n",
      "data_channel_is_socmed                0.000000\n",
      "data_channel_is_tech                  0.000000\n",
      "data_channel_is_world                 0.000000\n",
      "kw_min_min                           -1.000000\n",
      "kw_max_min                          660.000000\n",
      "kw_avg_min                          235.500000\n",
      "kw_min_max                         1400.000000\n",
      "kw_max_max                       843300.000000\n",
      "kw_avg_max                       244572.222223\n",
      "kw_min_avg                         1023.635611\n",
      "kw_max_avg                         4355.688836\n",
      "kw_avg_avg                         2870.074878\n",
      "self_reference_min_shares          1200.000000\n",
      "self_reference_max_shares          2800.000000\n",
      "self_reference_avg_sharess         2200.000000\n",
      "weekday_is_monday                     0.000000\n",
      "weekday_is_tuesday                    0.000000\n",
      "weekday_is_wednesday                  0.000000\n",
      "weekday_is_thursday                   0.000000\n",
      "weekday_is_friday                     0.000000\n",
      "weekday_is_saturday                   0.000000\n",
      "weekday_is_sunday                     0.000000\n",
      "is_weekend                            0.000000\n",
      "LDA_00                                0.033387\n",
      "LDA_01                                0.033345\n",
      "LDA_02                                0.040004\n",
      "LDA_03                                0.040001\n",
      "LDA_04                                0.040727\n",
      "global_subjectivity                   0.453457\n",
      "global_sentiment_polarity             0.119117\n",
      "global_rate_positive_words            0.039023\n",
      "global_rate_negative_words            0.015337\n",
      "rate_positive_words                   0.710526\n",
      "rate_negative_words                   0.280000\n",
      "avg_positive_polarity                 0.358755\n",
      "min_positive_polarity                 0.100000\n",
      "max_positive_polarity                 0.800000\n",
      "avg_negative_polarity                -0.253333\n",
      "min_negative_polarity                -0.500000\n",
      "max_negative_polarity                -0.100000\n",
      "title_subjectivity                    0.150000\n",
      "title_sentiment_polarity              0.000000\n",
      "abs_title_subjectivity                0.500000\n",
      "abs_title_sentiment_polarity          0.000000\n",
      "shares                             1400.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#calculate median for each column\n",
    "median = df.median(axis=0)\n",
    "avg = df.mean(axis=0)\n",
    "print(avg)\n",
    "print(median)\n",
    "# compute the median of each attribute\n",
    "medians = df.median()\n",
    "\n",
    "# discretize each attribute to 0 or 1 based on the median\n",
    "# for column in df.columns:\n",
    "#     df[column] = (df[column] >= medians[column]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T23:38:10.270961Z",
     "end_time": "2023-04-03T23:38:10.335280Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = np.array(df)\n",
    "if NORMALIZE:\n",
    "    # normalize the data\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "x = data[:, :-1]\n",
    "# converting the last column to boolean\n",
    "if BINARY_LABEL:\n",
    "    if not NORMALIZE:\n",
    "        y = np.array([elem >= 1400 for elem in data[:, -1]])\n",
    "    else:\n",
    "        y = np.array([e >= 0 for e in data[:, -1]])  #TODO check\n",
    "else:\n",
    "    y = np.array(data[:, -1])\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "dLIoAQQY-ql_",
    "outputId": "bf5d14ac-b0f6-410e-98cd-d69cea64e8ea",
    "ExecuteTime": {
     "start_time": "2023-04-04T00:15:57.129249Z",
     "end_time": "2023-04-04T00:15:57.176412Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def analyze_pred(pred, truth):\n",
    "    # Print the mean squared error and R-squared score\n",
    "    print('Mean Squared Error:', mean_squared_error(truth, pred))\n",
    "    print('R-squared Score:', r2_score(test_y, pred))\n",
    "    print(np.mean(pred))\n",
    "    print(np.mean(truth))\n",
    "    bin_pred = pred >= 0\n",
    "    bin_truth = truth >= 0\n",
    "    print('Accuracy:', accuracy_score(bin_truth, bin_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:06:04.046276Z",
     "end_time": "2023-04-04T00:06:04.048307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def analyze_pred_bin(pred, truth):\n",
    "    # Print the mean squared error and R-squared score\n",
    "    print('Binary cross entropy:', log_loss(truth, pred))\n",
    "    print(np.mean(pred))\n",
    "    print(np.mean(truth))\n",
    "    # bin_pred = pred >= 0\n",
    "    # bin_truth = truth >= 0\n",
    "    print('Accuracy:', accuracy_score(truth, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:06:04.049970Z",
     "end_time": "2023-04-04T00:06:04.052906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def test_model(model, train_x, train_y, test_x, test_y,classification=False):\n",
    "    time_start = time.time()\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Time taken to train the model: \", time.time() - time_start)\n",
    "    pred = model.predict(test_x)\n",
    "    if classification:\n",
    "        analyze_pred_bin(pred, test_y)\n",
    "    else:\n",
    "        analyze_pred(pred, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:06:04.054731Z",
     "end_time": "2023-04-04T00:06:04.057036Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  0.10197067260742188\n",
      "Mean Squared Error: 0.5051645325256792\n",
      "R-squared Score: 0.03273223844671258\n",
      "0.006017241700748466\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.6391726573338378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor =LinearRegression()\n",
    "test_model(regressor, train_x, train_y, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:22:31.387683Z",
     "end_time": "2023-04-04T00:22:31.536746Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56036005\n",
      "Iteration 2, loss = 0.56009221\n",
      "Iteration 3, loss = 0.55916890\n",
      "Iteration 4, loss = 0.55715870\n",
      "Iteration 5, loss = 0.55430329\n",
      "Iteration 6, loss = 0.55256319\n",
      "Iteration 7, loss = 0.55153655\n",
      "Iteration 8, loss = 0.55122578\n",
      "Iteration 9, loss = 0.55072943\n",
      "Iteration 10, loss = 0.55064699\n",
      "Iteration 11, loss = 0.55012299\n",
      "Iteration 12, loss = 0.55002449\n",
      "Iteration 13, loss = 0.55004426\n",
      "Iteration 14, loss = 0.54999184\n",
      "Iteration 15, loss = 0.54988152\n",
      "Iteration 16, loss = 0.54959622\n",
      "Iteration 17, loss = 0.54953497\n",
      "Iteration 18, loss = 0.54953586\n",
      "Iteration 19, loss = 0.54968835\n",
      "Iteration 20, loss = 0.54956565\n",
      "Iteration 21, loss = 0.54940212\n",
      "Iteration 22, loss = 0.54958317\n",
      "Iteration 23, loss = 0.54926492\n",
      "Iteration 24, loss = 0.54922762\n",
      "Iteration 25, loss = 0.54911666\n",
      "Iteration 26, loss = 0.54919690\n",
      "Iteration 27, loss = 0.54890509\n",
      "Iteration 28, loss = 0.54925859\n",
      "Iteration 29, loss = 0.54918348\n",
      "Iteration 30, loss = 0.54914147\n",
      "Iteration 31, loss = 0.54909393\n",
      "Iteration 32, loss = 0.54898969\n",
      "Iteration 33, loss = 0.54879304\n",
      "Iteration 34, loss = 0.54916068\n",
      "Iteration 35, loss = 0.54897904\n",
      "Iteration 36, loss = 0.54896370\n",
      "Iteration 37, loss = 0.54870743\n",
      "Iteration 38, loss = 0.54897385\n",
      "Iteration 39, loss = 0.54886773\n",
      "Iteration 40, loss = 0.54892202\n",
      "Iteration 41, loss = 0.54875508\n",
      "Iteration 42, loss = 0.54877814\n",
      "Iteration 43, loss = 0.54854813\n",
      "Iteration 44, loss = 0.54861199\n",
      "Iteration 45, loss = 0.54850979\n",
      "Iteration 46, loss = 0.54875789\n",
      "Iteration 47, loss = 0.54848353\n",
      "Iteration 48, loss = 0.54844420\n",
      "Iteration 49, loss = 0.54872608\n",
      "Iteration 50, loss = 0.54864120\n",
      "Iteration 51, loss = 0.54849496\n",
      "Iteration 52, loss = 0.54836606\n",
      "Iteration 53, loss = 0.54835992\n",
      "Iteration 54, loss = 0.54860799\n",
      "Iteration 55, loss = 0.54849763\n",
      "Iteration 56, loss = 0.54823369\n",
      "Iteration 57, loss = 0.54829842\n",
      "Iteration 58, loss = 0.54847529\n",
      "Iteration 59, loss = 0.54842695\n",
      "Iteration 60, loss = 0.54822563\n",
      "Iteration 61, loss = 0.54843089\n",
      "Iteration 62, loss = 0.54834143\n",
      "Iteration 63, loss = 0.54831768\n",
      "Iteration 64, loss = 0.54818586\n",
      "Iteration 65, loss = 0.54847343\n",
      "Iteration 66, loss = 0.54841945\n",
      "Iteration 67, loss = 0.54817648\n",
      "Iteration 68, loss = 0.54827296\n",
      "Iteration 69, loss = 0.54837643\n",
      "Iteration 70, loss = 0.54819849\n",
      "Iteration 71, loss = 0.54823565\n",
      "Iteration 72, loss = 0.54824317\n",
      "Iteration 73, loss = 0.54824433\n",
      "Iteration 74, loss = 0.54824898\n",
      "Iteration 75, loss = 0.54808667\n",
      "Iteration 76, loss = 0.54821322\n",
      "Iteration 77, loss = 0.54834028\n",
      "Training loss did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
      "Time taken to train the model:  221.82470154762268\n",
      "Mean Squared Error: 0.505126774004284\n",
      "R-squared Score: 0.032804536872508394\n",
      "-0.0011314285586095337\n",
      "-0.01342905790679993\n",
      "Accuracy: 0.671585319712448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 100, 100),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "    tol=0.0001,\n",
    "    verbose=True,\n",
    "    warm_start=False,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    n_iter_no_change=10,\n",
    "    max_fun=15000\n",
    ")\n",
    "regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(400, 300, 200,20),\n",
    "    activation='logistic',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.0001,\n",
    "    power_t=0.4,\n",
    "    max_iter=500,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "    tol=0.0001,\n",
    "    verbose=True,\n",
    "    warm_start=False,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.8,\n",
    "    beta_2=0.9,\n",
    "    epsilon=1e-08,\n",
    "    n_iter_no_change=20,\n",
    "    max_fun=15000\n",
    ")\n",
    "test_model(regressor, train_x, train_y, test_x, test_y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T01:16:03.786726Z",
     "end_time": "2023-04-04T01:19:45.842854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  61.37081599235535\n",
      "Binary cross entropy: 10.287146881015525\n",
      "0.2113759616597301\n",
      "0.20267372934796318\n",
      "Accuracy: 0.7145920040358179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier= MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 100, 100),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "    tol=0.0001,\n",
    "    verbose=False,\n",
    "    warm_start=False,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    n_iter_no_change=10,\n",
    "    max_fun=15000\n",
    ")\n",
    "test_model(classifier, train_x, train_y, test_x, test_y,classification=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T00:13:18.289844Z",
     "end_time": "2023-04-04T00:14:19.744662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
